[
  {
    "file_path": "embeddings.py",
    "name": "EmbeddingGenerator",
    "type": "class_definition",
    "start_line": 6,
    "end_line": 39,
    "docstring": "",
    "code": "class EmbeddingGenerator:\n    def __init__(self):\n        self.client = OpenAI(api_key=cfg.OPENAI_API_KEY)\n        self.model = cfg.EMBEDDING_MODEL\n        self.tokenizer = tiktoken.encoding_for_model(self.model)\n        self.max_tokens = cfg.MAX_CHUNK_TOKENS\n    \n    def generate(self, texts: Union[str, List[str]]) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings for one or multiple texts.\n        Handles batching and token limit validation.\n        \"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        \n        # Validate token counts\n        for i, text in enumerate(texts):\n            tokens = len(self.tokenizer.encode(text))\n            if tokens > self.max_tokens:\n                raise ValueError(\n                    f\"Text {i} has {tokens} tokens, exceeds limit of {self.max_tokens}. \"\n                    f\"First 100 chars: {text[:100]}...\"\n                )\n        \n        response = self.client.embeddings.create(\n            model=self.model,\n            input=texts\n        )\n        \n        return [data.embedding for data in response.data]\n    \n    def count_tokens(self, text: str) -> int:\n        \"\"\"Helper to check token count before embedding\"\"\"\n        return len(self.tokenizer.encode(text))",
    "enriched_text": "A class named 'Embedding Generator' at line 6 in module 'embeddings.py' located in folder '.'. File imports: from openai import OpenAI, from typing import List, Union, import tiktoken This class provides EmbeddingGenerator functionality with methods including: __init__ (implements function_definition logic), generate (implements function_definition logic), count_tokens (implements function_definition logic)."
  },
  {
    "file_path": "embeddings.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 7,
    "end_line": 11,
    "docstring": "",
    "code": "def __init__(self):\n        self.client = OpenAI(api_key=cfg.OPENAI_API_KEY)\n        self.model = cfg.EMBEDDING_MODEL\n        self.tokenizer = tiktoken.encoding_for_model(self.model)\n        self.max_tokens = cfg.MAX_CHUNK_TOKENS",
    "enriched_text": "A method named 'init' at line 7 in module 'embeddings.py' located in folder '.'. This is nested in EmbeddingGenerator. This method belongs to class 'EmbeddingGenerator'. File imports: from openai import OpenAI, from typing import List, Union, import tiktoken This function definition implements function_definition logic."
  },
  {
    "file_path": "embeddings.py",
    "name": "generate",
    "type": "function_definition",
    "start_line": 13,
    "end_line": 35,
    "docstring": "\n        Generate embeddings for one or multiple texts.\n        Handles batching and token limit validation.\n        ",
    "code": "def generate(self, texts: Union[str, List[str]]) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings for one or multiple texts.\n        Handles batching and token limit validation.\n        \"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        \n        # Validate token counts\n        for i, text in enumerate(texts):\n            tokens = len(self.tokenizer.encode(text))\n            if tokens > self.max_tokens:\n                raise ValueError(\n                    f\"Text {i} has {tokens} tokens, exceeds limit of {self.max_tokens}. \"\n                    f\"First 100 chars: {text[:100]}...\"\n                )\n        \n        response = self.client.embeddings.create(\n            model=self.model,\n            input=texts\n        )\n        \n        return [data.embedding for data in response.data]",
    "enriched_text": "A method named 'generate' at line 13 in module 'embeddings.py' located in folder '.'. This is nested in EmbeddingGenerator. This method belongs to class 'EmbeddingGenerator'. Docstring: Generate embeddings for one or multiple texts. File imports: from openai import OpenAI, from typing import List, Union, import tiktoken This function definition implements function_definition logic."
  },
  {
    "file_path": "embeddings.py",
    "name": "count_tokens",
    "type": "function_definition",
    "start_line": 37,
    "end_line": 39,
    "docstring": "Helper to check token count before embedding",
    "code": "def count_tokens(self, text: str) -> int:\n        \"\"\"Helper to check token count before embedding\"\"\"\n        return len(self.tokenizer.encode(text))",
    "enriched_text": "A method named 'count tokens' at line 37 in module 'embeddings.py' located in folder '.'. This is nested in EmbeddingGenerator. This method belongs to class 'EmbeddingGenerator'. Docstring: Helper to check token count before embedding File imports: from openai import OpenAI, from typing import List, Union, import tiktoken This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "FolderIndexer",
    "type": "class_definition",
    "start_line": 15,
    "end_line": 220,
    "docstring": "\n    Indexes an entire folder of Python files into Qdrant.\n    Handles errors per-file so one bad file doesn't crash the whole process.\n    ",
    "code": "class FolderIndexer:\n    \"\"\"\n    Indexes an entire folder of Python files into Qdrant.\n    Handles errors per-file so one bad file doesn't crash the whole process.\n    \"\"\"\n    \n    DEFAULT_EXCLUDES = [\n        \"__pycache__\",\n        \"venv\", \".venv\", \"env\", \".env\",\n        \".git\", \"node_modules\", \"build\", \"dist\",\n        \"*.pyc\", \"*.pyo\", \"*.pyd\", \"*.so\",\n        \"test_*.py\", \"*_test.py\", \"tests/\", \"test/\",\n    ]\n    \n    def __init__(\n        self,\n        folder_path: Path,\n        storage: QdrantStorage,\n        embedder: EmbeddingGenerator,\n        exclude_patterns: List[str] = None,\n        export_path: Optional[Path] = None\n    ):\n        self.folder_path = folder_path\n        self.chunker = CodeChunker(folder_path)\n        self.enricher = ChunkEnricher()\n        self.storage = storage\n        self.embedder = embedder\n        self.exclude_patterns = exclude_patterns or self.DEFAULT_EXCLUDES\n        \n        self.console = Console()\n        self.stats = {\n            \"files_processed\": 0,\n            \"files_skipped\": 0,\n            \"chunks_created\": 0,\n            \"embedding_tokens\": 0,\n            \"errors\": [],\n        }\n\n        self.export_path = export_path\n        self.export_chunks = []\n    \n    def index_folder(self, batch_size: int = 50) -> Dict[str, Any]:\n        \"\"\"\n        Recursively index all Python files in a folder.\n        \n        Args:\n            folder_path: Root folder to index\n            batch_size: How many files to embed in one API call\n            \n        Returns:\n            Statistics dictionary\n        \"\"\"\n        if not self.folder_path.is_dir():\n            raise ValueError(f\"Path is not a directory: {self.folder_path}\")\n        \n        # Discover all Python files\n        python_files = list(self._discover_files())\n        \n        if not python_files:\n            self.console.print(f\"[yellow]No Python files found in {self.folder_path}[/yellow]\")\n            return self.stats\n        \n        self.console.print(f\"[blue]Found {len(python_files)} Python files to index[/blue]\")\n        \n        # Process files with progress bar\n        with Progress() as progress:\n            task = progress.add_task(\"[cyan]Indexing files...\", total=len(python_files))\n\n            # Process in batches to avoid memory issues\n            for batch_start in range(0, len(python_files), batch_size):\n                batch = python_files[batch_start:batch_start + batch_size]\n                self._process_batch(batch, progress, task)\n        \n        self._print_summary()\n        \n        if self.export_path and self.export_chunks:\n            self._export_chunks()\n        \n        return self.stats\n    \n    \n    def _discover_files(self) -> List[Path]:\n        \"\"\"Find all Python files, respecting exclude patterns\"\"\"\n        python_files = []\n        \n        for py_file in self.folder_path.rglob(\"*.py\"):\n            # Skip excluded paths\n            if self._should_exclude(py_file):\n                continue\n            \n            # Skip empty files\n            if py_file.stat().st_size == 0:\n                continue\n                \n            python_files.append(py_file)\n        \n        return python_files\n    \n    def _should_exclude(self, file_path: Path) -> bool:\n        \"\"\"Check if file/path matches exclude patterns\"\"\"\n        path_str = str(file_path)\n        \n        for pattern in self.exclude_patterns:\n            if pattern in path_str:\n                return True\n        \n        return False\n    \n    def _process_batch(self, batch: List[Path], progress: Progress, task: TaskID):\n        \"\"\"Process a batch of files\"\"\"\n        all_chunks = []\n        \n        for file_path in batch:\n            try:\n                # Parse and enrich\n                chunks = self._process_file(file_path)\n                all_chunks.extend(chunks)\n                \n                self.stats[\"files_processed\"] += 1\n                progress.update(task, advance=1)\n                \n            except Exception as e:\n                self.stats[\"errors\"].append({\n                    \"file\": str(file_path),\n                    \"error\": str(e),\n                    \"traceback\": traceback.format_exc()\n                })\n                self.console.print(f\"[red]\u274c Failed {file_path}: {e}[/red]\")\n                progress.update(task, advance=1)\n        \n        # Batch generate embeddings\n        if all_chunks:\n            self._embed_and_store(all_chunks)\n    \n    def _process_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Parse and enrich a single file\"\"\"\n        chunks = self.chunker.parse_file(file_path)\n        \n        for chunk in chunks:\n            chunk[\"enriched_text\"] = self.enricher.enrich(chunk)\n            \n            # Add chunk to export list (before embeddings)\n            if self.export_path:\n                self.export_chunks.append(chunk.copy())  # Copy to avoid mutations\n        \n        return chunks\n    \n    def _embed_and_store(self, chunks: List[Dict[str, Any]]):\n        \"\"\"Generate embeddings and store in Qdrant\"\"\"\n        # Extract enriched texts\n        enriched_texts = [c[\"enriched_text\"] for c in chunks]\n        \n        # Count tokens\n        total_tokens = sum(\n            self.embedder.count_tokens(text) \n            for text in enriched_texts\n        )\n        self.stats[\"embedding_tokens\"] += total_tokens\n        \n        # Generate embeddings in one batch API call\n        vectors = self.embedder.generate(enriched_texts)\n        \n        # Store\n        stored_count = self.storage.upsert(chunks, vectors)\n        self.stats[\"chunks_created\"] += stored_count\n    \n    def _print_summary(self):\n        \"\"\"Print indexing statistics\"\"\"\n        self.console.print(\"\\n\" + \"=\"*60)\n        self.console.print(\"[bold green]INDEXING COMPLETE[/bold green]\")\n        self.console.print(\"=\"*60)\n        self.console.print(f\"\ud83d\udcc1 Files processed: {self.stats['files_processed']}\")\n        self.console.print(f\"\ud83d\udcc4 Chunks created: {self.stats['chunks_created']}\")\n        self.console.print(f\"\ud83e\ude99 Embedding tokens: {self.stats['embedding_tokens']:,.0f}\")\n        \n        if self.stats[\"files_skipped\"]:\n            self.console.print(f\"\u23ed\ufe0f  Files skipped: {self.stats['files_skipped']}\")\n        \n        if self.stats[\"errors\"]:\n            self.console.print(f\"\\n[red]\u274c Errors ({len(self.stats['errors'])})[/red]\")\n            for error in self.stats[\"errors\"][:3]:  # Show first 3\n                self.console.print(f\"   {error['file']}: {error['error']}\")\n        \n        self.console.print(\"=\"*60)\n\n    def _export_chunks(self):\n        \"\"\"Export all chunks to JSON file\"\"\"\n        export_data = []\n        \n        for chunk in self.export_chunks:\n            export_data.append({\n                \"file_path\": chunk[\"file_path\"],\n                \"name\": chunk[\"name\"],\n                \"type\": chunk[\"type\"],\n                \"start_line\": chunk[\"start_line\"],\n                \"end_line\": chunk[\"end_line\"],\n                \"docstring\": chunk[\"docstring\"],\n                \"code\": chunk[\"code\"],\n                \"enriched_text\": chunk[\"enriched_text\"]\n            })\n        \n        # Write to file\n        self.export_path.parent.mkdir(parents=True, exist_ok=True)\n        self.export_path.write_text(json.dumps(export_data, indent=2))\n        \n        self.console.print(f\"\\n[green]\ud83d\udcc4 Exported {len(export_data)} chunks to {self.export_path}[/green]\")",
    "enriched_text": "A class named 'Folder Indexer' at line 15 in module 'indexer.py' located in folder '.'. Docstring: Indexes an entire folder of Python files into Qdrant. File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This class provides FolderIndexer functionality with methods including: __init__ (implements function_definition logic), index_folder (implements function_definition logic), _discover_files (implements function_definition logic)."
  },
  {
    "file_path": "indexer.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 29,
    "end_line": 54,
    "docstring": "",
    "code": "def __init__(\n        self,\n        folder_path: Path,\n        storage: QdrantStorage,\n        embedder: EmbeddingGenerator,\n        exclude_patterns: List[str] = None,\n        export_path: Optional[Path] = None\n    ):\n        self.folder_path = folder_path\n        self.chunker = CodeChunker(folder_path)\n        self.enricher = ChunkEnricher()\n        self.storage = storage\n        self.embedder = embedder\n        self.exclude_patterns = exclude_patterns or self.DEFAULT_EXCLUDES\n        \n        self.console = Console()\n        self.stats = {\n            \"files_processed\": 0,\n            \"files_skipped\": 0,\n            \"chunks_created\": 0,\n            \"embedding_tokens\": 0,\n            \"errors\": [],\n        }\n\n        self.export_path = export_path\n        self.export_chunks = []",
    "enriched_text": "A method named 'init' at line 29 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "index_folder",
    "type": "function_definition",
    "start_line": 56,
    "end_line": 93,
    "docstring": "\n        Recursively index all Python files in a folder.\n        \n        Args:\n            folder_path: Root folder to index\n            batch_size: How many files to embed in one API call\n            \n        Returns:\n            Statistics dictionary\n        ",
    "code": "def index_folder(self, batch_size: int = 50) -> Dict[str, Any]:\n        \"\"\"\n        Recursively index all Python files in a folder.\n        \n        Args:\n            folder_path: Root folder to index\n            batch_size: How many files to embed in one API call\n            \n        Returns:\n            Statistics dictionary\n        \"\"\"\n        if not self.folder_path.is_dir():\n            raise ValueError(f\"Path is not a directory: {self.folder_path}\")\n        \n        # Discover all Python files\n        python_files = list(self._discover_files())\n        \n        if not python_files:\n            self.console.print(f\"[yellow]No Python files found in {self.folder_path}[/yellow]\")\n            return self.stats\n        \n        self.console.print(f\"[blue]Found {len(python_files)} Python files to index[/blue]\")\n        \n        # Process files with progress bar\n        with Progress() as progress:\n            task = progress.add_task(\"[cyan]Indexing files...\", total=len(python_files))\n\n            # Process in batches to avoid memory issues\n            for batch_start in range(0, len(python_files), batch_size):\n                batch = python_files[batch_start:batch_start + batch_size]\n                self._process_batch(batch, progress, task)\n        \n        self._print_summary()\n        \n        if self.export_path and self.export_chunks:\n            self._export_chunks()\n        \n        return self.stats",
    "enriched_text": "A method named 'index folder' at line 56 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Recursively index all Python files in a folder. File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_discover_files",
    "type": "function_definition",
    "start_line": 96,
    "end_line": 111,
    "docstring": "Find all Python files, respecting exclude patterns",
    "code": "def _discover_files(self) -> List[Path]:\n        \"\"\"Find all Python files, respecting exclude patterns\"\"\"\n        python_files = []\n        \n        for py_file in self.folder_path.rglob(\"*.py\"):\n            # Skip excluded paths\n            if self._should_exclude(py_file):\n                continue\n            \n            # Skip empty files\n            if py_file.stat().st_size == 0:\n                continue\n                \n            python_files.append(py_file)\n        \n        return python_files",
    "enriched_text": "A method named 'discover files' at line 96 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Find all Python files, respecting exclude patterns File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_should_exclude",
    "type": "function_definition",
    "start_line": 113,
    "end_line": 121,
    "docstring": "Check if file/path matches exclude patterns",
    "code": "def _should_exclude(self, file_path: Path) -> bool:\n        \"\"\"Check if file/path matches exclude patterns\"\"\"\n        path_str = str(file_path)\n        \n        for pattern in self.exclude_patterns:\n            if pattern in path_str:\n                return True\n        \n        return False",
    "enriched_text": "A method named 'should exclude' at line 113 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Check if file/path matches exclude patterns File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_process_batch",
    "type": "function_definition",
    "start_line": 123,
    "end_line": 147,
    "docstring": "Process a batch of files",
    "code": "def _process_batch(self, batch: List[Path], progress: Progress, task: TaskID):\n        \"\"\"Process a batch of files\"\"\"\n        all_chunks = []\n        \n        for file_path in batch:\n            try:\n                # Parse and enrich\n                chunks = self._process_file(file_path)\n                all_chunks.extend(chunks)\n                \n                self.stats[\"files_processed\"] += 1\n                progress.update(task, advance=1)\n                \n            except Exception as e:\n                self.stats[\"errors\"].append({\n                    \"file\": str(file_path),\n                    \"error\": str(e),\n                    \"traceback\": traceback.format_exc()\n                })\n                self.console.print(f\"[red]\u274c Failed {file_path}: {e}[/red]\")\n                progress.update(task, advance=1)\n        \n        # Batch generate embeddings\n        if all_chunks:\n            self._embed_and_store(all_chunks)",
    "enriched_text": "A method named 'process batch' at line 123 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Process a batch of files File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_process_file",
    "type": "function_definition",
    "start_line": 149,
    "end_line": 160,
    "docstring": "Parse and enrich a single file",
    "code": "def _process_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Parse and enrich a single file\"\"\"\n        chunks = self.chunker.parse_file(file_path)\n        \n        for chunk in chunks:\n            chunk[\"enriched_text\"] = self.enricher.enrich(chunk)\n            \n            # Add chunk to export list (before embeddings)\n            if self.export_path:\n                self.export_chunks.append(chunk.copy())  # Copy to avoid mutations\n        \n        return chunks",
    "enriched_text": "A method named 'process file' at line 149 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Parse and enrich a single file File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_embed_and_store",
    "type": "function_definition",
    "start_line": 162,
    "end_line": 179,
    "docstring": "Generate embeddings and store in Qdrant",
    "code": "def _embed_and_store(self, chunks: List[Dict[str, Any]]):\n        \"\"\"Generate embeddings and store in Qdrant\"\"\"\n        # Extract enriched texts\n        enriched_texts = [c[\"enriched_text\"] for c in chunks]\n        \n        # Count tokens\n        total_tokens = sum(\n            self.embedder.count_tokens(text) \n            for text in enriched_texts\n        )\n        self.stats[\"embedding_tokens\"] += total_tokens\n        \n        # Generate embeddings in one batch API call\n        vectors = self.embedder.generate(enriched_texts)\n        \n        # Store\n        stored_count = self.storage.upsert(chunks, vectors)\n        self.stats[\"chunks_created\"] += stored_count",
    "enriched_text": "A method named 'embed and store' at line 162 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Generate embeddings and store in Qdrant File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_print_summary",
    "type": "function_definition",
    "start_line": 181,
    "end_line": 198,
    "docstring": "Print indexing statistics",
    "code": "def _print_summary(self):\n        \"\"\"Print indexing statistics\"\"\"\n        self.console.print(\"\\n\" + \"=\"*60)\n        self.console.print(\"[bold green]INDEXING COMPLETE[/bold green]\")\n        self.console.print(\"=\"*60)\n        self.console.print(f\"\ud83d\udcc1 Files processed: {self.stats['files_processed']}\")\n        self.console.print(f\"\ud83d\udcc4 Chunks created: {self.stats['chunks_created']}\")\n        self.console.print(f\"\ud83e\ude99 Embedding tokens: {self.stats['embedding_tokens']:,.0f}\")\n        \n        if self.stats[\"files_skipped\"]:\n            self.console.print(f\"\u23ed\ufe0f  Files skipped: {self.stats['files_skipped']}\")\n        \n        if self.stats[\"errors\"]:\n            self.console.print(f\"\\n[red]\u274c Errors ({len(self.stats['errors'])})[/red]\")\n            for error in self.stats[\"errors\"][:3]:  # Show first 3\n                self.console.print(f\"   {error['file']}: {error['error']}\")\n        \n        self.console.print(\"=\"*60)",
    "enriched_text": "A method named 'print summary' at line 181 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Print indexing statistics File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "indexer.py",
    "name": "_export_chunks",
    "type": "function_definition",
    "start_line": 200,
    "end_line": 220,
    "docstring": "Export all chunks to JSON file",
    "code": "def _export_chunks(self):\n        \"\"\"Export all chunks to JSON file\"\"\"\n        export_data = []\n        \n        for chunk in self.export_chunks:\n            export_data.append({\n                \"file_path\": chunk[\"file_path\"],\n                \"name\": chunk[\"name\"],\n                \"type\": chunk[\"type\"],\n                \"start_line\": chunk[\"start_line\"],\n                \"end_line\": chunk[\"end_line\"],\n                \"docstring\": chunk[\"docstring\"],\n                \"code\": chunk[\"code\"],\n                \"enriched_text\": chunk[\"enriched_text\"]\n            })\n        \n        # Write to file\n        self.export_path.parent.mkdir(parents=True, exist_ok=True)\n        self.export_path.write_text(json.dumps(export_data, indent=2))\n        \n        self.console.print(f\"\\n[green]\ud83d\udcc4 Exported {len(export_data)} chunks to {self.export_path}[/green]\")",
    "enriched_text": "A method named 'export chunks' at line 200 in module 'indexer.py' located in folder '.'. This is nested in FolderIndexer. This method belongs to class 'FolderIndexer'. Docstring: Export all chunks to JSON file File imports: from pathlib import Path, from typing import List, Dict, Any, Tuple, Optional, import json This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "QdrantStorage",
    "type": "class_definition",
    "start_line": 9,
    "end_line": 109,
    "docstring": "",
    "code": "class QdrantStorage:\n    def __init__(self):\n        self.client = QdrantClient(\n            url=cfg.QDRANT_URL,\n            api_key=cfg.QDRANT_API_KEY,\n            prefer_grpc=False  # Use HTTP for compatibility\n        )\n        self.collection_name = cfg.COLLECTION_NAME\n        self._ensure_collection()\n    \n    def _ensure_collection(self):\n        \"\"\"Create collection if it doesn't exist\"\"\"\n        collections = self.client.get_collections().collections\n        if not any(c.name == self.collection_name for c in collections):\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=cfg.EMBEDDING_DIMENSIONS,\n                    distance=Distance.COSINE\n                )\n            )\n            print(f\"Created Qdrant collection: {self.collection_name}\")\n    \n    def upsert(self, chunks: List[Dict[str, Any]], vectors: List[List[float]]):\n        \"\"\"Store chunks with deterministic IDs to prevent duplicates\"\"\"\n        points = []\n        for chunk, vector in zip(chunks, vectors):\n            # Deterministic ID: hash of file_path:start_line\n            id_string = f\"{chunk['file_path']}:{chunk['start_line']}\"\n            point_id = hashlib.md5(id_string.encode()).hexdigest()\n            \n            payload = {\n                \"name\": chunk[\"name\"],\n                \"code\": chunk[\"code\"],\n                \"file_path\": chunk[\"file_path\"],\n                \"start_line\": chunk[\"start_line\"],\n                \"end_line\": chunk[\"end_line\"],\n                \"docstring\": chunk[\"docstring\"],\n                \"type\": chunk[\"type\"],\n                \"entity_type\": chunk.get(\"entity_type\", chunk[\"type\"]),\n                \"enriched_text\": chunk[\"enriched_text\"],\n                \"folder_path\": chunk.get(\"folder_path\", \"\"),\n                \"is_test\": chunk.get(\"is_test\", False)\n            }\n            points.append(PointStruct(id=point_id, vector=vector, payload=payload))\n        \n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points\n        )\n        return len(points)\n    \n    def search(self, query_vector: List[float], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Vector search - returns raw results using query_points\"\"\"\n        try:\n            # Modern qdrant-client API (v1.x)\n            results = self.client.query_points(\n                collection_name=self.collection_name,\n                query=query_vector,\n                limit=limit,\n                with_payload=True\n            )\n            \n            return [\n                {\n                    \"id\": hit.id,\n                    \"score\": hit.score,\n                    \"payload\": hit.payload\n                }\n                for hit in results.points\n            ]\n        except Exception as e:\n            print(f\"Search error: {e}\")\n            return []\n    \n    def delete_by_file(self, file_path: str):\n        \"\"\"Delete all chunks from a file (for re-indexing)\"\"\"\n        self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"file_path\",\n                        match=rest.MatchValue(value=file_path)\n                    )\n                ]\n            )\n        )\n\n    def clear_collection(self):\n        \"\"\"Delete and recreate collection to clear all points (most reliable)\"\"\"\n        try:\n            # Delete entire collection\n            self.client.delete_collection(self.collection_name)\n            print(f\"\ud83d\uddd1\ufe0f  Deleted collection: {self.collection_name}\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Collection might not exist: {e}\")\n        \n        # Recreate it fresh\n        self._ensure_collection()\n        print(f\"\u2705 Recreated collection: {self.collection_name}\")",
    "enriched_text": "A class named 'Qdrant Storage' at line 9 in module 'storage.py' located in folder '.'. File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This class provides QdrantStorage functionality with methods including: __init__ (implements function_definition logic), _ensure_collection (implements function_definition logic), upsert (implements function_definition logic)."
  },
  {
    "file_path": "storage.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 10,
    "end_line": 17,
    "docstring": "",
    "code": "def __init__(self):\n        self.client = QdrantClient(\n            url=cfg.QDRANT_URL,\n            api_key=cfg.QDRANT_API_KEY,\n            prefer_grpc=False  # Use HTTP for compatibility\n        )\n        self.collection_name = cfg.COLLECTION_NAME\n        self._ensure_collection()",
    "enriched_text": "A method named 'init' at line 10 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "_ensure_collection",
    "type": "function_definition",
    "start_line": 19,
    "end_line": 30,
    "docstring": "Create collection if it doesn't exist",
    "code": "def _ensure_collection(self):\n        \"\"\"Create collection if it doesn't exist\"\"\"\n        collections = self.client.get_collections().collections\n        if not any(c.name == self.collection_name for c in collections):\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=cfg.EMBEDDING_DIMENSIONS,\n                    distance=Distance.COSINE\n                )\n            )\n            print(f\"Created Qdrant collection: {self.collection_name}\")",
    "enriched_text": "A method named 'ensure collection' at line 19 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. Docstring: Create collection if it doesn't exist File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "upsert",
    "type": "function_definition",
    "start_line": 32,
    "end_line": 59,
    "docstring": "Store chunks with deterministic IDs to prevent duplicates",
    "code": "def upsert(self, chunks: List[Dict[str, Any]], vectors: List[List[float]]):\n        \"\"\"Store chunks with deterministic IDs to prevent duplicates\"\"\"\n        points = []\n        for chunk, vector in zip(chunks, vectors):\n            # Deterministic ID: hash of file_path:start_line\n            id_string = f\"{chunk['file_path']}:{chunk['start_line']}\"\n            point_id = hashlib.md5(id_string.encode()).hexdigest()\n            \n            payload = {\n                \"name\": chunk[\"name\"],\n                \"code\": chunk[\"code\"],\n                \"file_path\": chunk[\"file_path\"],\n                \"start_line\": chunk[\"start_line\"],\n                \"end_line\": chunk[\"end_line\"],\n                \"docstring\": chunk[\"docstring\"],\n                \"type\": chunk[\"type\"],\n                \"entity_type\": chunk.get(\"entity_type\", chunk[\"type\"]),\n                \"enriched_text\": chunk[\"enriched_text\"],\n                \"folder_path\": chunk.get(\"folder_path\", \"\"),\n                \"is_test\": chunk.get(\"is_test\", False)\n            }\n            points.append(PointStruct(id=point_id, vector=vector, payload=payload))\n        \n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points\n        )\n        return len(points)",
    "enriched_text": "A method named 'upsert' at line 32 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. Docstring: Store chunks with deterministic IDs to prevent duplicates File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "search",
    "type": "function_definition",
    "start_line": 61,
    "end_line": 82,
    "docstring": "Vector search - returns raw results using query_points",
    "code": "def search(self, query_vector: List[float], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Vector search - returns raw results using query_points\"\"\"\n        try:\n            # Modern qdrant-client API (v1.x)\n            results = self.client.query_points(\n                collection_name=self.collection_name,\n                query=query_vector,\n                limit=limit,\n                with_payload=True\n            )\n            \n            return [\n                {\n                    \"id\": hit.id,\n                    \"score\": hit.score,\n                    \"payload\": hit.payload\n                }\n                for hit in results.points\n            ]\n        except Exception as e:\n            print(f\"Search error: {e}\")\n            return []",
    "enriched_text": "A method named 'search' at line 61 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. Docstring: Vector search - returns raw results using query_points File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "delete_by_file",
    "type": "function_definition",
    "start_line": 84,
    "end_line": 96,
    "docstring": "Delete all chunks from a file (for re-indexing)",
    "code": "def delete_by_file(self, file_path: str):\n        \"\"\"Delete all chunks from a file (for re-indexing)\"\"\"\n        self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"file_path\",\n                        match=rest.MatchValue(value=file_path)\n                    )\n                ]\n            )\n        )",
    "enriched_text": "A method named 'delete by file' at line 84 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. Docstring: Delete all chunks from a file (for re-indexing) File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "storage.py",
    "name": "clear_collection",
    "type": "function_definition",
    "start_line": 98,
    "end_line": 109,
    "docstring": "Delete and recreate collection to clear all points (most reliable)",
    "code": "def clear_collection(self):\n        \"\"\"Delete and recreate collection to clear all points (most reliable)\"\"\"\n        try:\n            # Delete entire collection\n            self.client.delete_collection(self.collection_name)\n            print(f\"\ud83d\uddd1\ufe0f  Deleted collection: {self.collection_name}\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Collection might not exist: {e}\")\n        \n        # Recreate it fresh\n        self._ensure_collection()\n        print(f\"\u2705 Recreated collection: {self.collection_name}\")",
    "enriched_text": "A method named 'clear collection' at line 98 in module 'storage.py' located in folder '.'. This is nested in QdrantStorage. This method belongs to class 'QdrantStorage'. Docstring: Delete and recreate collection to clear all points (most reliable) File imports: from qdrant_client import QdrantClient, from qdrant_client.models import VectorParams, Distance, PointStruct, from qdrant_client.http import models as rest This function definition implements function_definition logic."
  },
  {
    "file_path": "usage.py.py",
    "name": "storing",
    "type": "function_definition",
    "start_line": 12,
    "end_line": 29,
    "docstring": "",
    "code": "def storing(folder_path: Path, export_path: Path, reindex: bool = True):\n    embedder = EmbeddingGenerator()\n    storage = QdrantStorage()\n\n    print(f\"\\nIndexing folder: {folder_path}\")\n    indexer = FolderIndexer(\n        folder_path=folder_path,\n        storage=storage, \n        embedder=embedder,\n        export_path=export_path\n    )\n    if reindex:\n        storage.clear_collection()\n    stats = indexer.index_folder()\n\n    if stats[\"chunks_created\"] == 0:\n        print(\"\u274c No chunks were indexed. Check for errors above.\")\n        return",
    "enriched_text": "A function named 'storing' at line 12 in module 'usage.py.py' located in folder '.'. File imports: from typing import List, Dict, Any, Optional, Union, from pathlib import Path, import settings as cfg This function definition implements function_definition logic."
  },
  {
    "file_path": "usage.py.py",
    "name": "retrieving",
    "type": "function_definition",
    "start_line": 31,
    "end_line": 60,
    "docstring": "",
    "code": "def retrieving(query: Union[List[str], str], top_k: int,):\n    embedder = EmbeddingGenerator()\n    storage = QdrantStorage()\n    retriever = HybridRetriever(storage, embedder)\n    \n    print(f\"\\nretrieving...\")\n    for q in query:\n        print(f\"\\n{'='*60}\")\n        print(f\"\ud83d\udcdd Query: '{q}'\")\n        print(f\"{'='*60}\")\n        \n        results = retriever.retrieve(q, top_k=top_k)\n        \n        if not results:\n            print(\"   \u274c No results found\")\n            continue\n        \n        for i, hit in enumerate(results, 1):\n            payload = hit[\"payload\"]\n            print(f\"\\n   {i}. \ud83d\udd17 {payload['file_path']}:{payload['start_line']}\")\n            print(f\"      \ud83d\udcc1 Folder: {payload['folder_path']}\")  # \u2190 ADD\n            print(f\"      \ud83d\udcca Score: {hit['score']:.3f}\")  # \u2190 ADD: Show boosted score\n            print(f\"      \ud83c\udfaf {payload['entity_type']}: '{payload['name']}'\")  # \u2190 ADD: Show entity type\n            if payload.get(\"is_test\"):\n                print(f\"      \u26a0\ufe0f  TEST FILE\")  # \u2190 ADD: Flag test files\n            print(f\"      \ud83d\udca1 {payload['enriched_text']}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PIPELINE COMPLETE\")\n    print(\"=\"*60)",
    "enriched_text": "A function named 'retrieving' at line 31 in module 'usage.py.py' located in folder '.'. File imports: from typing import List, Dict, Any, Optional, Union, from pathlib import Path, import settings as cfg This function definition implements function_definition logic."
  },
  {
    "file_path": "usage.py.py",
    "name": "test_pipeline",
    "type": "function_definition",
    "start_line": 63,
    "end_line": 82,
    "docstring": "Test the full retrieval pipeline with folder indexing",
    "code": "def test_pipeline():\n    \"\"\"Test the full retrieval pipeline with folder indexing\"\"\"\n    # Step 1: Index entire folder\n    test_folder = Path(__file__).parent\n    export_path=Path(__file__).parent / \"chunks.json\"\n    reindex = True\n    storing(test_folder, export_path, reindex)\n    \n    # Step 2: Run test queries\n    test_queries = [\n        # \"How do I calculate price with tax?\",\n        # \"Where do we apply discounts?\",\n        # \"Show me order validation logic\",\n        # \"How to connect to database?\",\n        # \"What are the user validation rules?\",\n        \"how does the chunking work?\",\n        \"what model is being used for embedding?\",\n        \"how the enriching process work?\"\n    ]\n    retrieving(test_queries, 3)",
    "enriched_text": "A function named 'test pipeline' at line 63 in module 'usage.py.py' located in folder '.'. Docstring: Test the full retrieval pipeline with folder indexing File imports: from typing import List, Dict, Any, Optional, Union, from pathlib import Path, import settings as cfg This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "CodeChunker",
    "type": "class_definition",
    "start_line": 6,
    "end_line": 157,
    "docstring": "",
    "code": "class CodeChunker:\n    def __init__(self, root_path: Path):\n        # Build language parser once\n        self.lang = Language(tsp.language())\n        self.parser = Parser(self.lang)\n        self.root_path = root_path\n    \n    def parse_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"\n        Parse Python file into structured chunks.\n        Returns: List of chunk dicts with code, metadata, and tree-sitter node info.\n        \"\"\"\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n            \n        code_bytes = file_path.read_bytes()\n        tree = self.parser.parse(code_bytes)\n        \n        chunks = []\n        self._walk_tree(tree.root_node, code_bytes, file_path, chunks)\n        file_docstring = self._extract_file_docstring(tree.root_node, code_bytes)\n        imports = self._extract_imports(tree.root_node, code_bytes)\n        \n        for chunk in chunks:\n            chunk[\"file_docstring\"] = file_docstring\n            chunk[\"imports\"] = imports\n        return chunks\n    \n    def _walk_tree(self, node: Node, code_bytes: bytes, file_path: Path, chunks: List[Dict], parent_stack: List[str] = None):\n        parent_stack = parent_stack or []\n        \n        if node.type in (\"function_definition\", \"class_definition\"):\n            chunk = self._extract_chunk(node, code_bytes, file_path, parent_stack)\n            if chunk:\n                chunks.append(chunk)\n                \n                # For classes, recurse into their body with updated parent stack\n                if node.type == \"class_definition\":\n                    new_stack = parent_stack + [chunk[\"name\"]]\n                    body_node = node.child_by_field_name(\"body\")\n                    if body_node:\n                        for child in body_node.children:\n                            self._walk_tree(child, code_bytes, file_path, chunks, new_stack)\n                    return  # Don't recurse further into classes\n        \n        # Recurse into children\n        for child in node.children:\n            self._walk_tree(child, code_bytes, file_path, chunks, parent_stack)\n    \n    def _extract_chunk(self, node: Node, code_bytes: bytes, file_path: Path, parent_stack: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Extract code slice and metadata from a node\"\"\"\n        start_byte = node.start_byte\n        end_byte = node.end_byte\n        code = code_bytes[start_byte:end_byte].decode(\"utf-8\")\n        \n        # Extract name\n        name_node = node.child_by_field_name(\"name\")\n        name = name_node.text.decode(\"utf-8\") if name_node else \"unnamed\"\n        \n        # Extract docstring (if exists)\n        docstring = self._extract_docstring(node, code_bytes)\n\n        # For classes, extract method names\n        methods = []\n        if node.type == \"class_definition\":\n            body_node = node.child_by_field_name(\"body\")\n            if body_node:\n                for child in body_node.children:\n                    if child.type == \"function_definition\":\n                        method_name_node = child.child_by_field_name(\"name\")\n                        if method_name_node:\n                            methods.append(method_name_node.text.decode(\"utf-8\"))\n\n        if parent_stack:\n            qualified_name = \".\".join(parent_stack + [name])\n        else:\n            qualified_name = name\n\n        parent_context = f\"{'.'.join(parent_stack)}\" if parent_stack else \"\"\n\n        entity_type = \"method\" if parent_stack else node.type.replace(\"_definition\", \"\")\n\n        # Detect test functions more accurately\n        is_test_func = (\n            name.startswith(\"test_\") or \n            name.endswith(\"_test\") or \n            \"test\" in name.lower() and \"unittest\" in (docstring or \"\").lower()\n        )\n        \n        # Detect test files\n        file_path_lower = str(file_path).lower()\n        is_test_file = any(x in file_path_lower for x in [\n            \"test_\", \"_test\", \"tests/\", \"/test\", \"conftest.py\", \"usage.py\"  # \u2190 Add usage.py here if it's your test file\n        ])\n\n        return {\n            \"name\": name,\n            \"type\": node.type,  # \"function_definition\" or \"class_definition\"\n            \"code\": code,\n            \"file_path\": str(file_path.relative_to(self.root_path)),\n            \"start_line\": node.start_point[0] + 1,\n            \"end_line\": node.end_point[0] + 1,\n            \"docstring\": docstring,\n            \"byte_range\": (start_byte, end_byte),\n            \"methods\": methods,\n            \"qualified_name\": qualified_name,\n            \"parent_class\": parent_stack[-1] if parent_stack else None,\n            \"parent_context\": parent_context,\n            \"parent_stack\": parent_stack or [],\n            \"entity_type\": entity_type,\n            \"folder_path\": str(file_path.parent.relative_to(self.root_path)),\n            \"is_test\": is_test_func or is_test_file\n        }\n    \n    def _extract_docstring(self, node: Node, code_bytes: bytes) -> str:\n        \"\"\"Extract docstring from function/class body\"\"\"\n        body_node = node.child_by_field_name(\"body\")\n        if not body_node or not body_node.children:\n            return \"\"\n        \n        # Look for first string expression\n        for child in body_node.children:\n            if child.type == \"expression_statement\":\n                expr = child.child(0)\n                if expr and expr.type == \"string\":\n                    return expr.text.decode(\"utf-8\").strip('\"\\'')\n        \n        return \"\"\n    \n    def _extract_imports(self, root_node: Node, code_bytes: bytes) -> List[str]:\n        \"\"\"Extract import statements for context\"\"\"\n        imports = []\n        \n        def walk(node):\n            if node.type in (\"import_statement\", \"import_from_statement\"):\n                imports.append(code_bytes[node.start_byte:node.end_byte].decode(\"utf-8\").strip())\n            \n            for child in node.children:\n                walk(child)\n        \n        walk(root_node)\n        return imports\n\n    def _extract_file_docstring(self, root_node: Node, code_bytes: bytes) -> str:\n        \"\"\"Extract module-level docstring\"\"\"\n        # Look for first expression statement with a string\n        for child in root_node.children:\n            if child.type == \"expression_statement\":\n                expr = child.child(0)\n                if expr and expr.type == \"string\":\n                    return expr.text.decode(\"utf-8\").strip('\"\\'')\n        return \"\"",
    "enriched_text": "A class named 'Code Chunker' at line 6 in module 'chunking.py' located in folder '.'. File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This class provides CodeChunker functionality with methods including: __init__ (implements function_definition logic), parse_file (implements function_definition logic), _walk_tree (implements function_definition logic)."
  },
  {
    "file_path": "chunking.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 7,
    "end_line": 11,
    "docstring": "",
    "code": "def __init__(self, root_path: Path):\n        # Build language parser once\n        self.lang = Language(tsp.language())\n        self.parser = Parser(self.lang)\n        self.root_path = root_path",
    "enriched_text": "A method named 'init' at line 7 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "parse_file",
    "type": "function_definition",
    "start_line": 13,
    "end_line": 32,
    "docstring": "\n        Parse Python file into structured chunks.\n        Returns: List of chunk dicts with code, metadata, and tree-sitter node info.\n        ",
    "code": "def parse_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"\n        Parse Python file into structured chunks.\n        Returns: List of chunk dicts with code, metadata, and tree-sitter node info.\n        \"\"\"\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n            \n        code_bytes = file_path.read_bytes()\n        tree = self.parser.parse(code_bytes)\n        \n        chunks = []\n        self._walk_tree(tree.root_node, code_bytes, file_path, chunks)\n        file_docstring = self._extract_file_docstring(tree.root_node, code_bytes)\n        imports = self._extract_imports(tree.root_node, code_bytes)\n        \n        for chunk in chunks:\n            chunk[\"file_docstring\"] = file_docstring\n            chunk[\"imports\"] = imports\n        return chunks",
    "enriched_text": "A method named 'parse file' at line 13 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. Docstring: Parse Python file into structured chunks. File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "_walk_tree",
    "type": "function_definition",
    "start_line": 34,
    "end_line": 53,
    "docstring": "",
    "code": "def _walk_tree(self, node: Node, code_bytes: bytes, file_path: Path, chunks: List[Dict], parent_stack: List[str] = None):\n        parent_stack = parent_stack or []\n        \n        if node.type in (\"function_definition\", \"class_definition\"):\n            chunk = self._extract_chunk(node, code_bytes, file_path, parent_stack)\n            if chunk:\n                chunks.append(chunk)\n                \n                # For classes, recurse into their body with updated parent stack\n                if node.type == \"class_definition\":\n                    new_stack = parent_stack + [chunk[\"name\"]]\n                    body_node = node.child_by_field_name(\"body\")\n                    if body_node:\n                        for child in body_node.children:\n                            self._walk_tree(child, code_bytes, file_path, chunks, new_stack)\n                    return  # Don't recurse further into classes\n        \n        # Recurse into children\n        for child in node.children:\n            self._walk_tree(child, code_bytes, file_path, chunks, parent_stack)",
    "enriched_text": "A method named 'walk tree' at line 34 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "_extract_chunk",
    "type": "function_definition",
    "start_line": 55,
    "end_line": 118,
    "docstring": "Extract code slice and metadata from a node",
    "code": "def _extract_chunk(self, node: Node, code_bytes: bytes, file_path: Path, parent_stack: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Extract code slice and metadata from a node\"\"\"\n        start_byte = node.start_byte\n        end_byte = node.end_byte\n        code = code_bytes[start_byte:end_byte].decode(\"utf-8\")\n        \n        # Extract name\n        name_node = node.child_by_field_name(\"name\")\n        name = name_node.text.decode(\"utf-8\") if name_node else \"unnamed\"\n        \n        # Extract docstring (if exists)\n        docstring = self._extract_docstring(node, code_bytes)\n\n        # For classes, extract method names\n        methods = []\n        if node.type == \"class_definition\":\n            body_node = node.child_by_field_name(\"body\")\n            if body_node:\n                for child in body_node.children:\n                    if child.type == \"function_definition\":\n                        method_name_node = child.child_by_field_name(\"name\")\n                        if method_name_node:\n                            methods.append(method_name_node.text.decode(\"utf-8\"))\n\n        if parent_stack:\n            qualified_name = \".\".join(parent_stack + [name])\n        else:\n            qualified_name = name\n\n        parent_context = f\"{'.'.join(parent_stack)}\" if parent_stack else \"\"\n\n        entity_type = \"method\" if parent_stack else node.type.replace(\"_definition\", \"\")\n\n        # Detect test functions more accurately\n        is_test_func = (\n            name.startswith(\"test_\") or \n            name.endswith(\"_test\") or \n            \"test\" in name.lower() and \"unittest\" in (docstring or \"\").lower()\n        )\n        \n        # Detect test files\n        file_path_lower = str(file_path).lower()\n        is_test_file = any(x in file_path_lower for x in [\n            \"test_\", \"_test\", \"tests/\", \"/test\", \"conftest.py\", \"usage.py\"  # \u2190 Add usage.py here if it's your test file\n        ])\n\n        return {\n            \"name\": name,\n            \"type\": node.type,  # \"function_definition\" or \"class_definition\"\n            \"code\": code,\n            \"file_path\": str(file_path.relative_to(self.root_path)),\n            \"start_line\": node.start_point[0] + 1,\n            \"end_line\": node.end_point[0] + 1,\n            \"docstring\": docstring,\n            \"byte_range\": (start_byte, end_byte),\n            \"methods\": methods,\n            \"qualified_name\": qualified_name,\n            \"parent_class\": parent_stack[-1] if parent_stack else None,\n            \"parent_context\": parent_context,\n            \"parent_stack\": parent_stack or [],\n            \"entity_type\": entity_type,\n            \"folder_path\": str(file_path.parent.relative_to(self.root_path)),\n            \"is_test\": is_test_func or is_test_file\n        }",
    "enriched_text": "A method named 'extract chunk' at line 55 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. Docstring: Extract code slice and metadata from a node File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "_extract_docstring",
    "type": "function_definition",
    "start_line": 120,
    "end_line": 133,
    "docstring": "Extract docstring from function/class body",
    "code": "def _extract_docstring(self, node: Node, code_bytes: bytes) -> str:\n        \"\"\"Extract docstring from function/class body\"\"\"\n        body_node = node.child_by_field_name(\"body\")\n        if not body_node or not body_node.children:\n            return \"\"\n        \n        # Look for first string expression\n        for child in body_node.children:\n            if child.type == \"expression_statement\":\n                expr = child.child(0)\n                if expr and expr.type == \"string\":\n                    return expr.text.decode(\"utf-8\").strip('\"\\'')\n        \n        return \"\"",
    "enriched_text": "A method named 'extract docstring' at line 120 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. Docstring: Extract docstring from function/class body File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "_extract_imports",
    "type": "function_definition",
    "start_line": 135,
    "end_line": 147,
    "docstring": "Extract import statements for context",
    "code": "def _extract_imports(self, root_node: Node, code_bytes: bytes) -> List[str]:\n        \"\"\"Extract import statements for context\"\"\"\n        imports = []\n        \n        def walk(node):\n            if node.type in (\"import_statement\", \"import_from_statement\"):\n                imports.append(code_bytes[node.start_byte:node.end_byte].decode(\"utf-8\").strip())\n            \n            for child in node.children:\n                walk(child)\n        \n        walk(root_node)\n        return imports",
    "enriched_text": "A method named 'extract imports' at line 135 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. Docstring: Extract import statements for context File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "walk",
    "type": "function_definition",
    "start_line": 139,
    "end_line": 144,
    "docstring": "",
    "code": "def walk(node):\n            if node.type in (\"import_statement\", \"import_from_statement\"):\n                imports.append(code_bytes[node.start_byte:node.end_byte].decode(\"utf-8\").strip())\n            \n            for child in node.children:\n                walk(child)",
    "enriched_text": "A method named 'walk' at line 139 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "chunking.py",
    "name": "_extract_file_docstring",
    "type": "function_definition",
    "start_line": 149,
    "end_line": 157,
    "docstring": "Extract module-level docstring",
    "code": "def _extract_file_docstring(self, root_node: Node, code_bytes: bytes) -> str:\n        \"\"\"Extract module-level docstring\"\"\"\n        # Look for first expression statement with a string\n        for child in root_node.children:\n            if child.type == \"expression_statement\":\n                expr = child.child(0)\n                if expr and expr.type == \"string\":\n                    return expr.text.decode(\"utf-8\").strip('\"\\'')\n        return \"\"",
    "enriched_text": "A method named 'extract file docstring' at line 149 in module 'chunking.py' located in folder '.'. This is nested in CodeChunker. This method belongs to class 'CodeChunker'. Docstring: Extract module-level docstring File imports: from typing import List, Dict, Any, from pathlib import Path, import tree_sitter_python as tsp This function definition implements function_definition logic."
  },
  {
    "file_path": "enrichment.py",
    "name": "ChunkEnricher",
    "type": "class_definition",
    "start_line": 5,
    "end_line": 118,
    "docstring": "Enrich code chunks with natural language context for embeddings",
    "code": "class ChunkEnricher:\n    \"\"\"Enrich code chunks with natural language context for embeddings\"\"\"\n    \n    def __init__(self):\n        # Common acronyms to preserve\n        self.acronyms = {\n            \"api\", \"http\", \"url\", \"uuid\", \"id\", \"db\", \"sql\", \"json\", \n            \"xml\", \"html\", \"css\", \"js\", \"jwt\", \"oauth\", \"rest\"\n        }\n\n        # Customize this template based on your codebase style\n        self.template = (\n            \"A {chunk_type} named '{name}' defined in file '{file_path}' \"\n            \"at line {start_line}. \"\n            \"{docstring_summary} \"\n            \"This {chunk_type} {purpose}.\"\n        )\n    \n    def enrich(self, chunk: Dict[str, Any]) -> str:\n        # Normalize identifiers\n        name_norm = self._normalize_identifier(chunk[\"name\"])\n        \n        # Build parts\n        parts = [\n            f\"A {chunk['entity_type']} named '{name_norm}' at line {chunk['start_line']} in module '{chunk['file_path']}' located in folder '{chunk['folder_path']}'.\",\n        ]\n        \n        # Add parent context\n        if chunk.get(\"parent_context\"):\n            parts.append(f\"This is nested in {chunk['parent_context']}.\")\n\n        if chunk[\"parent_class\"]:\n            parts.append(f\"This method belongs to class '{chunk['parent_class']}'.\")\n        \n        # Add docstring\n        if chunk.get(\"docstring\"):\n            first_line = chunk[\"docstring\"].strip().split(\"\\n\")[0]\n            parts.append(f\"Docstring: {first_line}\")\n        \n        # Add imports for context\n        if chunk.get(\"imports\"):\n            import_list = \", \".join(chunk[\"imports\"][:3])\n            parts.append(f\"File imports: {import_list}\")\n        \n        # Add purpose/responsibility\n        if chunk[\"type\"] == \"class_definition\":\n            parts.append(self._extract_class_responsibility(chunk))\n        else:\n            purpose = self._infer_purpose(chunk[\"name\"], chunk[\"type\"])\n            parts.append(f\"This {chunk['type'].replace('_', ' ')} {purpose}.\")\n        \n        # Join with spaces\n        enriched = \" \".join(parts)\n        \n        # Clean up whitespace\n        return \" \".join(enriched.split())\n    \n    def _infer_purpose(self, name: str, chunk_type: str) -> str:\n        \"\"\"Simple heuristic to guess purpose from naming conventions\"\"\"\n        name_lower = name.lower()\n        \n        if chunk_type == \"function\":\n            if \"get\" in name_lower or \"fetch\" in name_lower:\n                return \"retrieves data\"\n            elif \"set\" in name_lower or \"update\" in name_lower:\n                return \"modifies data\"\n            elif \"calc\" in name_lower or \"compute\" in name_lower:\n                return \"performs a calculation\"\n            elif \"validate\" in name_lower or \"check\" in name_lower:\n                return \"validates input\"\n            elif \"handle\" in name_lower or \"process\" in name_lower:\n                return \"handles a specific operation\"\n        \n        return f\"implements {chunk_type} logic\"\n    \n    def _normalize_identifier(self, identifier: str) -> str:\n        \"\"\"Convert code identifiers to natural language, preserving acronyms\"\"\"\n        # Split camelCase: HTTPClient -> HTTP Client\n        identifier = re.sub(\n            r'([a-z])([A-Z])', \n            r'\\1 \\2', \n            identifier\n        )\n        \n        # Handle snake_case: calculate_total_price -> calculate total price\n        identifier = identifier.replace(\"_\", \" \")\n        \n        # Preserve acronyms: http client -> HTTP client, get api key -> get API key\n        words = identifier.split()\n        result = []\n        for word in words:\n            if word.lower() in self.acronyms:\n                result.append(word.upper())\n            else:\n                result.append(word)\n        \n        return \" \".join(result)\n    \n    def _extract_class_responsibility(self, chunk: Dict[str, Any]) -> str:\n        \"\"\"Generate a useful class description based on its methods\"\"\"\n        if not chunk.get(\"methods\"):\n            return f\"This class implements {chunk['name']} functionality.\"\n        \n        # Summarize method purposes\n        method_purposes = []\n        for method in chunk[\"methods\"][:3]:  # Top 3 methods\n            purpose = self._infer_purpose(method, \"function_definition\")\n            method_purposes.append(f\"{method} ({purpose})\")\n        \n        if method_purposes:\n            return (f\"This class provides {chunk['name']} functionality with methods \"\n                    f\"including: {', '.join(method_purposes)}.\")\n        \n        return f\"This class implements {chunk['name']}.\"",
    "enriched_text": "A class named 'Chunk Enricher' at line 5 in module 'enrichment.py' located in folder '.'. Docstring: Enrich code chunks with natural language context for embeddings File imports: import inflection, import re, from typing import Dict, Any This class provides ChunkEnricher functionality with methods including: __init__ (implements function_definition logic), enrich (implements function_definition logic), _infer_purpose (implements function_definition logic)."
  },
  {
    "file_path": "enrichment.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 8,
    "end_line": 21,
    "docstring": "",
    "code": "def __init__(self):\n        # Common acronyms to preserve\n        self.acronyms = {\n            \"api\", \"http\", \"url\", \"uuid\", \"id\", \"db\", \"sql\", \"json\", \n            \"xml\", \"html\", \"css\", \"js\", \"jwt\", \"oauth\", \"rest\"\n        }\n\n        # Customize this template based on your codebase style\n        self.template = (\n            \"A {chunk_type} named '{name}' defined in file '{file_path}' \"\n            \"at line {start_line}. \"\n            \"{docstring_summary} \"\n            \"This {chunk_type} {purpose}.\"\n        )",
    "enriched_text": "A method named 'init' at line 8 in module 'enrichment.py' located in folder '.'. This is nested in ChunkEnricher. This method belongs to class 'ChunkEnricher'. File imports: import inflection, import re, from typing import Dict, Any This function definition implements function_definition logic."
  },
  {
    "file_path": "enrichment.py",
    "name": "enrich",
    "type": "function_definition",
    "start_line": 23,
    "end_line": 60,
    "docstring": "",
    "code": "def enrich(self, chunk: Dict[str, Any]) -> str:\n        # Normalize identifiers\n        name_norm = self._normalize_identifier(chunk[\"name\"])\n        \n        # Build parts\n        parts = [\n            f\"A {chunk['entity_type']} named '{name_norm}' at line {chunk['start_line']} in module '{chunk['file_path']}' located in folder '{chunk['folder_path']}'.\",\n        ]\n        \n        # Add parent context\n        if chunk.get(\"parent_context\"):\n            parts.append(f\"This is nested in {chunk['parent_context']}.\")\n\n        if chunk[\"parent_class\"]:\n            parts.append(f\"This method belongs to class '{chunk['parent_class']}'.\")\n        \n        # Add docstring\n        if chunk.get(\"docstring\"):\n            first_line = chunk[\"docstring\"].strip().split(\"\\n\")[0]\n            parts.append(f\"Docstring: {first_line}\")\n        \n        # Add imports for context\n        if chunk.get(\"imports\"):\n            import_list = \", \".join(chunk[\"imports\"][:3])\n            parts.append(f\"File imports: {import_list}\")\n        \n        # Add purpose/responsibility\n        if chunk[\"type\"] == \"class_definition\":\n            parts.append(self._extract_class_responsibility(chunk))\n        else:\n            purpose = self._infer_purpose(chunk[\"name\"], chunk[\"type\"])\n            parts.append(f\"This {chunk['type'].replace('_', ' ')} {purpose}.\")\n        \n        # Join with spaces\n        enriched = \" \".join(parts)\n        \n        # Clean up whitespace\n        return \" \".join(enriched.split())",
    "enriched_text": "A method named 'enrich' at line 23 in module 'enrichment.py' located in folder '.'. This is nested in ChunkEnricher. This method belongs to class 'ChunkEnricher'. File imports: import inflection, import re, from typing import Dict, Any This function definition implements function_definition logic."
  },
  {
    "file_path": "enrichment.py",
    "name": "_infer_purpose",
    "type": "function_definition",
    "start_line": 62,
    "end_line": 78,
    "docstring": "Simple heuristic to guess purpose from naming conventions",
    "code": "def _infer_purpose(self, name: str, chunk_type: str) -> str:\n        \"\"\"Simple heuristic to guess purpose from naming conventions\"\"\"\n        name_lower = name.lower()\n        \n        if chunk_type == \"function\":\n            if \"get\" in name_lower or \"fetch\" in name_lower:\n                return \"retrieves data\"\n            elif \"set\" in name_lower or \"update\" in name_lower:\n                return \"modifies data\"\n            elif \"calc\" in name_lower or \"compute\" in name_lower:\n                return \"performs a calculation\"\n            elif \"validate\" in name_lower or \"check\" in name_lower:\n                return \"validates input\"\n            elif \"handle\" in name_lower or \"process\" in name_lower:\n                return \"handles a specific operation\"\n        \n        return f\"implements {chunk_type} logic\"",
    "enriched_text": "A method named 'infer purpose' at line 62 in module 'enrichment.py' located in folder '.'. This is nested in ChunkEnricher. This method belongs to class 'ChunkEnricher'. Docstring: Simple heuristic to guess purpose from naming conventions File imports: import inflection, import re, from typing import Dict, Any This function definition implements function_definition logic."
  },
  {
    "file_path": "enrichment.py",
    "name": "_normalize_identifier",
    "type": "function_definition",
    "start_line": 80,
    "end_line": 101,
    "docstring": "Convert code identifiers to natural language, preserving acronyms",
    "code": "def _normalize_identifier(self, identifier: str) -> str:\n        \"\"\"Convert code identifiers to natural language, preserving acronyms\"\"\"\n        # Split camelCase: HTTPClient -> HTTP Client\n        identifier = re.sub(\n            r'([a-z])([A-Z])', \n            r'\\1 \\2', \n            identifier\n        )\n        \n        # Handle snake_case: calculate_total_price -> calculate total price\n        identifier = identifier.replace(\"_\", \" \")\n        \n        # Preserve acronyms: http client -> HTTP client, get api key -> get API key\n        words = identifier.split()\n        result = []\n        for word in words:\n            if word.lower() in self.acronyms:\n                result.append(word.upper())\n            else:\n                result.append(word)\n        \n        return \" \".join(result)",
    "enriched_text": "A method named 'normalize identifier' at line 80 in module 'enrichment.py' located in folder '.'. This is nested in ChunkEnricher. This method belongs to class 'ChunkEnricher'. Docstring: Convert code identifiers to natural language, preserving acronyms File imports: import inflection, import re, from typing import Dict, Any This function definition implements function_definition logic."
  },
  {
    "file_path": "enrichment.py",
    "name": "_extract_class_responsibility",
    "type": "function_definition",
    "start_line": 103,
    "end_line": 118,
    "docstring": "Generate a useful class description based on its methods",
    "code": "def _extract_class_responsibility(self, chunk: Dict[str, Any]) -> str:\n        \"\"\"Generate a useful class description based on its methods\"\"\"\n        if not chunk.get(\"methods\"):\n            return f\"This class implements {chunk['name']} functionality.\"\n        \n        # Summarize method purposes\n        method_purposes = []\n        for method in chunk[\"methods\"][:3]:  # Top 3 methods\n            purpose = self._infer_purpose(method, \"function_definition\")\n            method_purposes.append(f\"{method} ({purpose})\")\n        \n        if method_purposes:\n            return (f\"This class provides {chunk['name']} functionality with methods \"\n                    f\"including: {', '.join(method_purposes)}.\")\n        \n        return f\"This class implements {chunk['name']}.\"",
    "enriched_text": "A method named 'extract class responsibility' at line 103 in module 'enrichment.py' located in folder '.'. This is nested in ChunkEnricher. This method belongs to class 'ChunkEnricher'. Docstring: Generate a useful class description based on its methods File imports: import inflection, import re, from typing import Dict, Any This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "HybridRetriever",
    "type": "class_definition",
    "start_line": 8,
    "end_line": 227,
    "docstring": "",
    "code": "class HybridRetriever:\n    def __init__(self, storage, embedder):\n        self.storage = storage\n        self.embedder = embedder\n        self.cross_encoder = CrossEncoder(cfg.CROSS_ENCODER_MODEL)\n        self.bm25 = None\n        self.bm25_chunks = []\n\n    def retrieve(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"\n        Full hybrid retrieval with hierarchical boosting.\n        \"\"\"\n        # Get more candidates to give boosting room to work\n        candidates = self.hybrid_retrieve(query, top_k=50)\n        \n        # Apply hierarchical boosting\n        boosted = self.hierarchical_boost(candidates)\n        \n        # Sort by boosted scores and take top candidates for reranking\n        boosted.sort(key=lambda x: x[\"score\"], reverse=True)\n        \n        # Rerank top candidates\n        reranked = self._rerank(query, boosted[:top_k * 2])\n        \n        return reranked[:top_k]\n    \n    def hybrid_retrieve(self, query: str, top_k: int = 10) -> List[Dict]:\n        \"\"\"Full hybrid retrieval with deduplication\"\"\"\n        query_vector = self.embedder.generate(query)[0]\n        \n        # Get candidates\n        semantic_results = self.storage.search(query_vector, limit=top_k * 3)\n        lexical_results = self._bm25_search(query, top_k * 3)\n        \n        # Combine and deduplicate\n        all_candidates = semantic_results + lexical_results\n        unique_candidates = self._deduplicate_by_content(all_candidates)\n        \n        # Rerank top unique candidates\n        unique_candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n        candidates_to_rerank = unique_candidates[:top_k * 2]\n        \n        reranked = self._rerank(query, candidates_to_rerank)\n        return reranked[:top_k]\n    \n    def hierarchical_boost(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Boost items that share hierarchy with top result, demote tests\"\"\"\n        if not candidates:\n            return candidates\n        \n        top_payload = candidates[0][\"payload\"]\n        top_folder = top_payload.get(\"folder_path\", \"\")\n        top_file = top_payload[\"file_path\"]\n        \n        for candidate in candidates:\n            payload = candidate[\"payload\"]\n            \n            # Same folder \u2192 1.5x boost\n            if payload.get(\"folder_path\", \"\") == top_folder:\n                candidate[\"score\"] *= cfg.SAME_FOLDER_BOOST\n            \n            # Same file \u2192 2x boost\n            if payload[\"file_path\"] == top_file:\n                candidate[\"score\"] *= cfg.SAME_FILE_BOOST\n            \n            # Test file \u2192 0.01x penalty (demote heavily)\n            if payload.get(\"is_test\", False):\n                candidate[\"score\"] *= cfg.TEST_PENALTY\n        \n        return candidates\n    \n    def _bm25_search(self, query: str, limit: int) -> List[Dict[str, Any]]:\n        \"\"\"Lexical search using BM25 on chunk names and code\"\"\"\n        if not self.bm25:\n            self._build_bm25_index()\n        \n        if not self.bm25:\n            return []\n        \n        query_tokens = query.lower().split()\n        scores = self.bm25.get_scores(query_tokens)\n        top_indices = np.argsort(scores)[-limit:][::-1]\n        \n        return [\n            {\n                \"id\": f\"bm25_{i}\",\n                \"score\": float(scores[i]),\n                \"payload\": self.bm25_chunks[i]\n            }\n            for i in top_indices if scores[i] > 0\n        ]\n    \n    def _build_bm25_index(self):\n        \"\"\"Build BM25 index from all chunks in Qdrant\"\"\"\n        all_points = []\n        offset = None\n        \n        while True:\n            try:\n                # Use scroll API\n                scroll_result = self.storage.client.scroll(\n                    collection_name=self.storage.collection_name,\n                    limit=100,\n                    offset=offset,\n                    with_payload=True\n                )\n                \n                # Handle both tuple and object return types\n                if isinstance(scroll_result, tuple):\n                    results, next_offset = scroll_result\n                else:\n                    # It's a ScrollResult object\n                    results = scroll_result.points\n                    next_offset = scroll_result.next_page_offset\n                \n                all_points.extend(results)\n                if not next_offset:\n                    break\n                offset = next_offset\n            except Exception as e:\n                print(f\"Scroll error: {e}\")\n                break\n        \n        corpus = []\n        self.bm25_chunks = []\n        \n        for point in all_points:\n            payload = point.payload\n            text = f\"{payload['name']} {payload['code']} {payload['file_path']}\"\n            corpus.append(text.lower().split())\n            self.bm25_chunks.append(payload)\n        \n        if corpus:\n            self.bm25 = BM25Okapi(corpus)\n    \n    def _merge_results(self, semantic: List[Dict], lexical: List[Dict]) -> List[Dict]:\n        \"\"\"Merge results using Reciprocal Rank Fusion\"\"\"\n        by_id = {}\n        \n        for rank, hit in enumerate(semantic):\n            by_id[hit[\"id\"]] = {\n                \"semantic_score\": hit[\"score\"],\n                \"lexical_score\": 0.0,\n                \"payload\": hit[\"payload\"],\n                \"semantic_rank\": rank\n            }\n        \n        for rank, hit in enumerate(lexical):\n            if hit[\"id\"] in by_id:\n                by_id[hit[\"id\"]][\"lexical_score\"] = hit[\"score\"]\n            else:\n                by_id[hit[\"id\"]] = {\n                    \"semantic_score\": 0.0,\n                    \"lexical_score\": hit[\"score\"],\n                    \"payload\": hit[\"payload\"],\n                    \"semantic_rank\": 999\n                }\n        \n        for item in by_id.values():\n            item[\"rrf_score\"] = self._rrf_score(item[\"semantic_score\"], item[\"semantic_rank\"], item[\"lexical_score\"])\n        \n        return sorted(\n            by_id.values(),\n            key=lambda x: x[\"rrf_score\"],\n            reverse=True\n        )\n    \n    def _rrf_score(self, semantic_score: float, semantic_rank: int, lexical_score: float, k: int = 60) -> float:\n        \"\"\"Reciprocal Rank Fusion scoring\"\"\"\n        semantic_rank_weight = 1 / (semantic_rank + k)\n        lexical_score_weight = lexical_score * 0.1\n        return semantic_score + lexical_score_weight + semantic_rank_weight\n    \n    def _rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank candidates using cross-encoder\"\"\"\n        if not candidates:\n            return []\n        \n        pairs = []\n        for hit in candidates:\n            payload = hit[\"payload\"]\n            chunk_text = (\n                f\"{payload['name']} - \"\n                f\"{payload['type']} - \"\n                f\"{payload.get('enriched_text', '')[:300]} - \"\n                f\"{payload['file_path']}:{payload['start_line']}\"\n            )\n            pairs.append([query, chunk_text])\n        \n        scores = self.cross_encoder.predict(pairs)\n\n        # print(f\"DEBUG - Query: '{query}'\")\n        # for i, (hit, score) in enumerate(zip(candidates, scores)):\n        #     print(f\"  {i+1}. {hit['payload']['name']}: {score:.3f}\")\n\n        import numpy as np\n        scores = np.exp(scores) / np.sum(np.exp(scores))\n        \n        scored_candidates = []\n        for hit, score in zip(candidates, scores):\n            hit_copy = hit.copy()  # Don't mutate original\n            hit_copy[\"rerank_score\"] = float(score * 10)\n            scored_candidates.append(hit_copy)\n        \n        return sorted(scored_candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n    \n    def _deduplicate_by_content(self, results: List[Dict]) -> List[Dict]:\n        \"\"\"Remove duplicates based on file_path + start_line\"\"\"\n        seen = set()\n        unique = []\n        \n        for hit in results:\n            payload = hit[\"payload\"]\n            key = (payload[\"file_path\"], payload[\"start_line\"])\n            \n            if key not in seen:\n                seen.add(key)\n                unique.append(hit)\n        \n        return unique",
    "enriched_text": "A class named 'Hybrid Retriever' at line 8 in module 'retrieval.py' located in folder '.'. File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This class provides HybridRetriever functionality with methods including: __init__ (implements function_definition logic), retrieve (implements function_definition logic), hybrid_retrieve (implements function_definition logic)."
  },
  {
    "file_path": "retrieval.py",
    "name": "__init__",
    "type": "function_definition",
    "start_line": 9,
    "end_line": 14,
    "docstring": "",
    "code": "def __init__(self, storage, embedder):\n        self.storage = storage\n        self.embedder = embedder\n        self.cross_encoder = CrossEncoder(cfg.CROSS_ENCODER_MODEL)\n        self.bm25 = None\n        self.bm25_chunks = []",
    "enriched_text": "A method named 'init' at line 9 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "retrieve",
    "type": "function_definition",
    "start_line": 16,
    "end_line": 32,
    "docstring": "\n        Full hybrid retrieval with hierarchical boosting.\n        ",
    "code": "def retrieve(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"\n        Full hybrid retrieval with hierarchical boosting.\n        \"\"\"\n        # Get more candidates to give boosting room to work\n        candidates = self.hybrid_retrieve(query, top_k=50)\n        \n        # Apply hierarchical boosting\n        boosted = self.hierarchical_boost(candidates)\n        \n        # Sort by boosted scores and take top candidates for reranking\n        boosted.sort(key=lambda x: x[\"score\"], reverse=True)\n        \n        # Rerank top candidates\n        reranked = self._rerank(query, boosted[:top_k * 2])\n        \n        return reranked[:top_k]",
    "enriched_text": "A method named 'retrieve' at line 16 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Full hybrid retrieval with hierarchical boosting. File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "hybrid_retrieve",
    "type": "function_definition",
    "start_line": 34,
    "end_line": 51,
    "docstring": "Full hybrid retrieval with deduplication",
    "code": "def hybrid_retrieve(self, query: str, top_k: int = 10) -> List[Dict]:\n        \"\"\"Full hybrid retrieval with deduplication\"\"\"\n        query_vector = self.embedder.generate(query)[0]\n        \n        # Get candidates\n        semantic_results = self.storage.search(query_vector, limit=top_k * 3)\n        lexical_results = self._bm25_search(query, top_k * 3)\n        \n        # Combine and deduplicate\n        all_candidates = semantic_results + lexical_results\n        unique_candidates = self._deduplicate_by_content(all_candidates)\n        \n        # Rerank top unique candidates\n        unique_candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n        candidates_to_rerank = unique_candidates[:top_k * 2]\n        \n        reranked = self._rerank(query, candidates_to_rerank)\n        return reranked[:top_k]",
    "enriched_text": "A method named 'hybrid retrieve' at line 34 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Full hybrid retrieval with deduplication File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "hierarchical_boost",
    "type": "function_definition",
    "start_line": 53,
    "end_line": 77,
    "docstring": "Boost items that share hierarchy with top result, demote tests",
    "code": "def hierarchical_boost(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Boost items that share hierarchy with top result, demote tests\"\"\"\n        if not candidates:\n            return candidates\n        \n        top_payload = candidates[0][\"payload\"]\n        top_folder = top_payload.get(\"folder_path\", \"\")\n        top_file = top_payload[\"file_path\"]\n        \n        for candidate in candidates:\n            payload = candidate[\"payload\"]\n            \n            # Same folder \u2192 1.5x boost\n            if payload.get(\"folder_path\", \"\") == top_folder:\n                candidate[\"score\"] *= cfg.SAME_FOLDER_BOOST\n            \n            # Same file \u2192 2x boost\n            if payload[\"file_path\"] == top_file:\n                candidate[\"score\"] *= cfg.SAME_FILE_BOOST\n            \n            # Test file \u2192 0.01x penalty (demote heavily)\n            if payload.get(\"is_test\", False):\n                candidate[\"score\"] *= cfg.TEST_PENALTY\n        \n        return candidates",
    "enriched_text": "A method named 'hierarchical boost' at line 53 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Boost items that share hierarchy with top result, demote tests File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_bm25_search",
    "type": "function_definition",
    "start_line": 79,
    "end_line": 98,
    "docstring": "Lexical search using BM25 on chunk names and code",
    "code": "def _bm25_search(self, query: str, limit: int) -> List[Dict[str, Any]]:\n        \"\"\"Lexical search using BM25 on chunk names and code\"\"\"\n        if not self.bm25:\n            self._build_bm25_index()\n        \n        if not self.bm25:\n            return []\n        \n        query_tokens = query.lower().split()\n        scores = self.bm25.get_scores(query_tokens)\n        top_indices = np.argsort(scores)[-limit:][::-1]\n        \n        return [\n            {\n                \"id\": f\"bm25_{i}\",\n                \"score\": float(scores[i]),\n                \"payload\": self.bm25_chunks[i]\n            }\n            for i in top_indices if scores[i] > 0\n        ]",
    "enriched_text": "A method named 'bm25 search' at line 79 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Lexical search using BM25 on chunk names and code File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_build_bm25_index",
    "type": "function_definition",
    "start_line": 100,
    "end_line": 141,
    "docstring": "Build BM25 index from all chunks in Qdrant",
    "code": "def _build_bm25_index(self):\n        \"\"\"Build BM25 index from all chunks in Qdrant\"\"\"\n        all_points = []\n        offset = None\n        \n        while True:\n            try:\n                # Use scroll API\n                scroll_result = self.storage.client.scroll(\n                    collection_name=self.storage.collection_name,\n                    limit=100,\n                    offset=offset,\n                    with_payload=True\n                )\n                \n                # Handle both tuple and object return types\n                if isinstance(scroll_result, tuple):\n                    results, next_offset = scroll_result\n                else:\n                    # It's a ScrollResult object\n                    results = scroll_result.points\n                    next_offset = scroll_result.next_page_offset\n                \n                all_points.extend(results)\n                if not next_offset:\n                    break\n                offset = next_offset\n            except Exception as e:\n                print(f\"Scroll error: {e}\")\n                break\n        \n        corpus = []\n        self.bm25_chunks = []\n        \n        for point in all_points:\n            payload = point.payload\n            text = f\"{payload['name']} {payload['code']} {payload['file_path']}\"\n            corpus.append(text.lower().split())\n            self.bm25_chunks.append(payload)\n        \n        if corpus:\n            self.bm25 = BM25Okapi(corpus)",
    "enriched_text": "A method named 'build bm25 index' at line 100 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Build BM25 index from all chunks in Qdrant File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_merge_results",
    "type": "function_definition",
    "start_line": 143,
    "end_line": 173,
    "docstring": "Merge results using Reciprocal Rank Fusion",
    "code": "def _merge_results(self, semantic: List[Dict], lexical: List[Dict]) -> List[Dict]:\n        \"\"\"Merge results using Reciprocal Rank Fusion\"\"\"\n        by_id = {}\n        \n        for rank, hit in enumerate(semantic):\n            by_id[hit[\"id\"]] = {\n                \"semantic_score\": hit[\"score\"],\n                \"lexical_score\": 0.0,\n                \"payload\": hit[\"payload\"],\n                \"semantic_rank\": rank\n            }\n        \n        for rank, hit in enumerate(lexical):\n            if hit[\"id\"] in by_id:\n                by_id[hit[\"id\"]][\"lexical_score\"] = hit[\"score\"]\n            else:\n                by_id[hit[\"id\"]] = {\n                    \"semantic_score\": 0.0,\n                    \"lexical_score\": hit[\"score\"],\n                    \"payload\": hit[\"payload\"],\n                    \"semantic_rank\": 999\n                }\n        \n        for item in by_id.values():\n            item[\"rrf_score\"] = self._rrf_score(item[\"semantic_score\"], item[\"semantic_rank\"], item[\"lexical_score\"])\n        \n        return sorted(\n            by_id.values(),\n            key=lambda x: x[\"rrf_score\"],\n            reverse=True\n        )",
    "enriched_text": "A method named 'merge results' at line 143 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Merge results using Reciprocal Rank Fusion File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_rrf_score",
    "type": "function_definition",
    "start_line": 175,
    "end_line": 179,
    "docstring": "Reciprocal Rank Fusion scoring",
    "code": "def _rrf_score(self, semantic_score: float, semantic_rank: int, lexical_score: float, k: int = 60) -> float:\n        \"\"\"Reciprocal Rank Fusion scoring\"\"\"\n        semantic_rank_weight = 1 / (semantic_rank + k)\n        lexical_score_weight = lexical_score * 0.1\n        return semantic_score + lexical_score_weight + semantic_rank_weight",
    "enriched_text": "A method named 'rrf score' at line 175 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Reciprocal Rank Fusion scoring File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_rerank",
    "type": "function_definition",
    "start_line": 181,
    "end_line": 212,
    "docstring": "Rerank candidates using cross-encoder",
    "code": "def _rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank candidates using cross-encoder\"\"\"\n        if not candidates:\n            return []\n        \n        pairs = []\n        for hit in candidates:\n            payload = hit[\"payload\"]\n            chunk_text = (\n                f\"{payload['name']} - \"\n                f\"{payload['type']} - \"\n                f\"{payload.get('enriched_text', '')[:300]} - \"\n                f\"{payload['file_path']}:{payload['start_line']}\"\n            )\n            pairs.append([query, chunk_text])\n        \n        scores = self.cross_encoder.predict(pairs)\n\n        # print(f\"DEBUG - Query: '{query}'\")\n        # for i, (hit, score) in enumerate(zip(candidates, scores)):\n        #     print(f\"  {i+1}. {hit['payload']['name']}: {score:.3f}\")\n\n        import numpy as np\n        scores = np.exp(scores) / np.sum(np.exp(scores))\n        \n        scored_candidates = []\n        for hit, score in zip(candidates, scores):\n            hit_copy = hit.copy()  # Don't mutate original\n            hit_copy[\"rerank_score\"] = float(score * 10)\n            scored_candidates.append(hit_copy)\n        \n        return sorted(scored_candidates, key=lambda x: x[\"rerank_score\"], reverse=True)",
    "enriched_text": "A method named 'rerank' at line 181 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Rerank candidates using cross-encoder File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  },
  {
    "file_path": "retrieval.py",
    "name": "_deduplicate_by_content",
    "type": "function_definition",
    "start_line": 214,
    "end_line": 227,
    "docstring": "Remove duplicates based on file_path + start_line",
    "code": "def _deduplicate_by_content(self, results: List[Dict]) -> List[Dict]:\n        \"\"\"Remove duplicates based on file_path + start_line\"\"\"\n        seen = set()\n        unique = []\n        \n        for hit in results:\n            payload = hit[\"payload\"]\n            key = (payload[\"file_path\"], payload[\"start_line\"])\n            \n            if key not in seen:\n                seen.add(key)\n                unique.append(hit)\n        \n        return unique",
    "enriched_text": "A method named 'deduplicate by content' at line 214 in module 'retrieval.py' located in folder '.'. This is nested in HybridRetriever. This method belongs to class 'HybridRetriever'. Docstring: Remove duplicates based on file_path + start_line File imports: from typing import List, Dict, Any, from rank_bm25 import BM25Okapi, from sentence_transformers import CrossEncoder This function definition implements function_definition logic."
  }
]